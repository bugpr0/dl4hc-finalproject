{\rtf1\ansi\ansicpg1252\cocoartf2707
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fnil\fcharset0 HelveticaNeue-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red52\green52\blue52;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c26494\c26499\c26494;\cssrgb\c100000\c100000\c100000;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\margt1598
\deftab720
\pard\tqr\tx9020\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \up0 \nosupersub \ulnone Sunday, March 12, 2023\
\pard\tqr\tx9020\pardeftab720\sa200\partightenfactor0
\cf2 \
\pard\pardeftab720\sa360\partightenfactor0

\f1\b\fs36 \cf3 DL4HC - Final Project Proposal - Team 61\
\pard\pardeftab720\sl288\slmult1\sa360\partightenfactor0

\f0\b0\fs28 \cf2 \kerning1\expnd1\expndtw5
Pre-training of Graph Augmented Transformers for Medication Recommendation \
\pard\pardeftab720\sl288\slmult1\sa40\partightenfactor0
\cf2 \kerning1\expnd1\expndtw5
We choose for our final project, the GBERT graph learning model described in the 2019 paper {\field{\*\fldinst{HYPERLINK "https://www.ijcai.org/proceedings/2019/825"}}{\fldrslt \kerning1\expnd1\expndtw5
\ul https://www.ijcai.org/proceedings/2019/825}} for the goal of drug recommendation.\
\pard\pardeftab720\sa160\partightenfactor0

\fs22 \cf2 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl288\slmult1\sa160\partightenfactor0

\fs24 \cf2 The paper "Pre-training of Graph Augmented Transformers for Medication Recommendation" aims to solve the problem of medication recommendation, which is a critical task in healthcare. The paper presents a new approach to this problem using pre-training of Graph Augmented Transformers (GATs) on a large corpus of medical texts.\
The authors argue that existing medication recommendation methods often ignore the complex relationships among medications, symptoms, and diseases, which can lead to suboptimal recommendations. To address this issue, the proposed model leverages medical knowledge encoded in large-scale medical corpora to pre-train the GAT model to learn the underlying structure of medical concepts and relationships.\
The pre-trained GAT model is then fine-tuned on patient medical records to personalize the medication recommendation for each patient. The model can capture not only the direct relationships between medications and symptoms but also the indirect relationships between medications through shared diseases or symptoms.\cb4 \
\cb1 The authors evaluate their proposed approach on several real-world datasets and show that it outperforms state-of-the-art medication recommendation methods. The MIMIC3 Dataset is used by the implementation of this paper and that is what the team will use for our coursework final project.\
The specific enhancements discussed are :\
\pard\tx20\tx196\pardeftab720\li196\fi-197\sl288\slmult1\sa160\partightenfactor0
\ls1\ilvl0\cf2 \up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone Pre-training to leverage more data - adapt the framework of BERT and pre-train the model on each visit of the EHR data to leverage the single-visit data that were not fit for training in other medication recommendation models.\
\ls1\ilvl0\up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone Medical ontology embedding with graph neural networks\
\ls1\ilvl0\up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone The framework of G-BERT consists of three main parts: ontology embedding, BERT and fine-tuned classifier. \
\pard\tx180\tx376\pardeftab720\li376\fi-377\sl288\slmult1\sa160\partightenfactor0
\ls1\ilvl1\cf2 \up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone Ontology embedding for medical code laid in leaf nodes by cooperating ancestors information based on graph attention networks.\
\ls1\ilvl1\up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone Next, input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pre- trained.\
\ls1\ilvl1\up0 \nosupersub \ulnone {\listtext	\uc0\u8226 	}\up0 \nosupersub \ulnone Finally, concatenate the mean of all previous visit embeddings and the last visit embedding as input and fine-tune the prediction layers for medication recommendation tasks.\
\pard\pardeftab720\sl288\slmult1\sa160\partightenfactor0
\cf2 The proposed approach is innovative because it addresses some of the key limitations of existing medication recommendation methods, such as the lack of consideration of complex relationships among medications, symptoms, and diseases. By using a pre-trained GAT model, the proposed approach can provide more personalized and effective medication recommendations for patients, leading to improved patient outcomes.\
As part of the project, we will be reproducing the results of the paper using the base code  {\field{\*\fldinst{HYPERLINK "https://github.com/jshang123/G-Bert"}}{\fldrslt \ul https://github.com/jshang123/G-Bert}}  on MIMIC3. As an extension we will apply the same on MIMIC IV dataset.TODO: Why is this interesting ? Computational feasibility ? \
}